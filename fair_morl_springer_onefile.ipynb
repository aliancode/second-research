{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4360136d-4bfd-4529-b402-3a26e6e92b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 16:58:54,806 - INFO - üöÄ Starting Springer-level MORL experiment with HIGH NOVELTY...\n",
      "2025-10-06 16:58:54,807 - INFO - üîç Loading EdNet-KT1 and inferring correctness...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Script loaded in Jupyter. Call `run_experiment()` to start.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferring correctness: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:03<00:00, 254.92it/s]\n",
      "Loading user data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:03<00:00, 231.66it/s]\n",
      "2025-10-06 16:59:05,204 - INFO - ‚úÖ Loaded 103,860 interactions from 287 students\n",
      "2025-10-06 16:59:05,219 - INFO - üß† Training DKT model...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================================================================\n",
    "#  CONFIGURATION (NO YAML)\n",
    "# =============================================================================\n",
    "EDNET_PATH = \"./EdNet-KT1/\"        \n",
    "MAX_USERS = 800\n",
    "MIN_INTERACTIONS = 20\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 8\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "CV_FOLDS = 3\n",
    "N_RECOMMENDATIONS = 5\n",
    "MAX_SEQ_LEN = 200\n",
    "FAIRNESS_GROUPS = [\"low\", \"medium\", \"high\"]\n",
    "WEIGHT_CONFIGS = [\n",
    "    [1.0, 0.0, 0.0],  # Mastery-only\n",
    "    [0.0, 1.0, 0.0],  # Engagement-only\n",
    "    [0.0, 0.0, 1.0],  # Fairness-only\n",
    "    [0.7, 0.2, 0.1],  # Mastery-focused\n",
    "    [0.5, 0.3, 0.2]   # Balanced\n",
    "]\n",
    "BOOTSTRAP_SAMPLES = 1000\n",
    "ALPHA = 0.05\n",
    "\n",
    "# =============================================================================\n",
    "# REPRODUCIBILITY & SETUP\n",
    "# =============================================================================\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "os.makedirs(\"outputs/figures\", exist_ok=True)\n",
    "os.makedirs(\"outputs/models\", exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"outputs/training.log\"),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# =============================================================================\n",
    "# DATA LOADING & CORRECTNESS INFERENCE\n",
    "# =============================================================================\n",
    "def load_and_preprocess():\n",
    "    logger.info(\"üîç Loading EdNet-KT1 and inferring correctness...\")\n",
    "    user_files = [f for f in os.listdir(EDNET_PATH) if f.startswith(\"u\") and f.endswith(\".csv\")]\n",
    "    if not user_files:\n",
    "        raise FileNotFoundError(f\"No .csv files in {EDNET_PATH}. Ensure it contains real KT1 data.\")\n",
    "    \n",
    "    np.random.shuffle(user_files)\n",
    "    user_files = user_files[:MAX_USERS]\n",
    "    \n",
    "    # Pass 1: Collect answers to infer correct responses\n",
    "    all_answers = []\n",
    "    valid_files = []\n",
    "    for f in tqdm(user_files, desc=\"Inferring correctness\"):\n",
    "        try:\n",
    "            df_user = pd.read_csv(os.path.join(EDNET_PATH, f))\n",
    "            if \"question_id\" in df_user.columns and \"user_answer\" in df_user.columns:\n",
    "                all_answers.append(df_user[[\"question_id\", \"user_answer\"]])\n",
    "                valid_files.append(f)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    if not all_answers:\n",
    "        raise ValueError(\"No valid KT1 files found. Ensure files have 'question_id' and 'user_answer'.\")\n",
    "    \n",
    "    # Infer correct answer as most frequent response per question\n",
    "    all_answers_df = pd.concat(all_answers, ignore_index=True)\n",
    "    answer_map = all_answers_df.groupby(\"question_id\")[\"user_answer\"].agg(\n",
    "        lambda x: Counter(x).most_common(1)[0][0]\n",
    "    ).to_dict()\n",
    "    \n",
    "    # Pass 2: Load data with inferred correctness\n",
    "    data_frames = []\n",
    "    for f in tqdm(valid_files, desc=\"Loading user data\"):\n",
    "        try:\n",
    "            df_user = pd.read_csv(os.path.join(EDNET_PATH, f))\n",
    "            df_user[\"user_id\"] = f.replace(\".csv\", \"\")\n",
    "            if \"question_id\" not in df_user.columns or \"user_answer\" not in df_user.columns:\n",
    "                continue\n",
    "            df_user[\"correct\"] = df_user.apply(\n",
    "                lambda row: 1 if row[\"user_answer\"] == answer_map.get(row[\"question_id\"], None) else 0,\n",
    "                axis=1\n",
    "            )\n",
    "            if len(df_user) >= MIN_INTERACTIONS:\n",
    "                data_frames.append(df_user)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    if not data_frames:\n",
    "        raise ValueError(\"No valid user data after correctness inference.\")\n",
    "    \n",
    "    df_raw = pd.concat(data_frames, ignore_index=True)\n",
    "    df = df_raw.dropna(subset=[\"question_id\", \"correct\"]).copy()\n",
    "    df[\"correct\"] = df[\"correct\"].astype(int)\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors='coerce')\n",
    "    df = df.sort_values([\"user_id\", \"timestamp\"]).reset_index(drop=True)\n",
    "    df[\"skill_id\"] = pd.Categorical(df[\"question_id\"]).codes\n",
    "    \n",
    "    # Skill groups (fairness proxy)\n",
    "    user_perf = df.groupby(\"user_id\")[\"correct\"].mean()\n",
    "    user_perf = pd.qcut(user_perf, q=3, labels=FAIRNESS_GROUPS)\n",
    "    df[\"skill_group\"] = df[\"user_id\"].map(user_perf)\n",
    "    \n",
    "    # Time features\n",
    "    df[\"time_delta\"] = df.groupby(\"user_id\")[\"timestamp\"].diff().dt.total_seconds().fillna(0)\n",
    "    df[\"time_delta\"] = df[\"time_delta\"].clip(0, 3600)\n",
    "    df[\"time_delta_norm\"] = df[\"time_delta\"] / 3600.0\n",
    "    \n",
    "    # Session entropy (engagement)\n",
    "    df[\"date\"] = df[\"timestamp\"].dt.date\n",
    "    df[\"session_id\"] = df.groupby([\"user_id\", \"date\"]).ngroup()\n",
    "    session_entropy = []\n",
    "    for session_id, session in df.groupby(\"session_id\"):\n",
    "        if len(session) == 0:\n",
    "            session_entropy.extend([0.0] * len(session))\n",
    "            continue\n",
    "        counts = Counter(session[\"skill_id\"])\n",
    "        total = sum(counts.values())\n",
    "        probs = np.array(list(counts.values())) / total\n",
    "        entropy = -np.sum(probs * np.log(probs + 1e-8))\n",
    "        session_entropy.extend([entropy] * len(session))\n",
    "    df[\"session_entropy\"] = session_entropy\n",
    "    \n",
    "    # Encode IDs\n",
    "    user_ids = {uid: idx for idx, uid in enumerate(df[\"user_id\"].unique())}\n",
    "    skill_ids = {sid: idx for idx, sid in enumerate(df[\"skill_id\"].unique())}\n",
    "    df[\"user_idx\"] = df[\"user_id\"].map(user_ids)\n",
    "    df[\"skill_idx\"] = df[\"skill_id\"].map(skill_ids)\n",
    "    \n",
    "    torch.save({\"user_ids\": user_ids, \"skill_ids\": skill_ids}, \"outputs/id_mappings.pth\")\n",
    "    logger.info(f\"‚úÖ Loaded {len(df):,} interactions from {df['user_id'].nunique()} students\")\n",
    "    return df, len(skill_ids)\n",
    "\n",
    "# =============================================================================\n",
    "# DEEP KNOWLEDGE TRACING (DKT)\n",
    "# =============================================================================\n",
    "class DeepKnowledgeTracing(nn.Module):\n",
    "    def __init__(self, n_skills, embed_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.n_skills = n_skills\n",
    "        self.skill_embed = nn.Embedding(n_skills * 2, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, n_skills)\n",
    "    def forward(self, skills, correct):\n",
    "        x = skills + correct.long() * self.n_skills\n",
    "        emb = self.skill_embed(x)\n",
    "        out, _ = self.lstm(emb)\n",
    "        return torch.sigmoid(self.fc(out))\n",
    "\n",
    "# =============================================================================\n",
    "# FAIRNESS-AWARE MORL AGENT\n",
    "# =============================================================================\n",
    "class FairMORLAgent(nn.Module):\n",
    "    def __init__(self, n_skills, embed_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.skill_embed = nn.Embedding(n_skills, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim + 5, hidden_dim, batch_first=True)\n",
    "        self.actor = nn.Linear(hidden_dim, n_skills)\n",
    "        self.critic = nn.Linear(hidden_dim, 3)\n",
    "    def forward(self, x):\n",
    "        skills = x[:, :, 0].long()\n",
    "        others = x[:, :, 1:]\n",
    "        emb = self.skill_embed(skills)\n",
    "        lstm_in = torch.cat([emb, others], dim=-1)\n",
    "        _, (h, _) = self.lstm(lstm_in)\n",
    "        h = h.squeeze(0)\n",
    "        return self.actor(h), self.critic(h)\n",
    "\n",
    "def compute_engagement_reward(session_actions, time_deltas):\n",
    "    if len(session_actions) == 0: return 0.0\n",
    "    counts = Counter(session_actions)\n",
    "    total = sum(counts.values())\n",
    "    probs = np.array(list(counts.values())) / total\n",
    "    entropy = -np.sum(probs * np.log(probs + 1e-8))\n",
    "    avg_dwell = np.mean(time_deltas) if len(time_deltas) > 0 else 0\n",
    "    length_norm = min(len(session_actions) / 20.0, 1.0)\n",
    "    return 0.4 * entropy + 0.3 * min(avg_dwell, 1.0) + 0.3 * length_norm\n",
    "\n",
    "# =============================================================================\n",
    "# FAIRNESS METRICS\n",
    "# =============================================================================\n",
    "class FairnessMetrics:\n",
    "    def __init__(self, fairness_groups):\n",
    "        self.fairness_groups = fairness_groups\n",
    "    \n",
    "    def statistical_parity_difference(self, group_exposures, n_skills):\n",
    "        exposures = []\n",
    "        for group in self.fairness_groups:\n",
    "            total = max(sum(group_exposures[group].values()), 1)\n",
    "            vec = np.array([group_exposures[group].get(s, 0) / total for s in range(n_skills)])\n",
    "            exposures.append(vec)\n",
    "        spd = 0.0\n",
    "        for i in range(len(exposures)):\n",
    "            for j in range(i+1, len(exposures)):\n",
    "                spd = max(spd, np.abs(exposures[i] - exposures[j]).max())\n",
    "        return spd\n",
    "    \n",
    "    def equal_opportunity_difference(self, y_true, y_pred, groups):\n",
    "        eop_values = []\n",
    "        for group in self.fairness_groups:\n",
    "            group_mask = (groups == group)\n",
    "            if np.sum(group_mask) == 0: continue\n",
    "            group_tpr = np.mean(y_pred[group_mask & (y_true == 1)] > 0.5)\n",
    "            eop_values.append(group_tpr)\n",
    "        return max(eop_values) - min(eop_values) if len(eop_values) >= 2 else 0.0\n",
    "    \n",
    "    def demographic_parity(self, recommendations, groups):\n",
    "        group_probs = []\n",
    "        for group in self.fairness_groups:\n",
    "            group_mask = (groups == group)\n",
    "            if np.sum(group_mask) == 0: continue\n",
    "            group_rec = np.array(recommendations)[group_mask]\n",
    "            group_probs.append(np.mean([len(set(rec)) for rec in group_rec]))\n",
    "        return max(group_probs) - min(group_probs) if len(group_probs) >= 2 else 0.0\n",
    "    \n",
    "    def individual_fairness_violation(self, recommendations, user_features, similarity_threshold=0.8):\n",
    "        violations = 0\n",
    "        total_pairs = 0\n",
    "        for i in range(len(recommendations)):\n",
    "            for j in range(i+1, len(recommendations)):\n",
    "                if len(user_features[i]) == 0 or len(user_features[j]) == 0:\n",
    "                    continue\n",
    "                similarity = np.dot(user_features[i], user_features[j]) / (\n",
    "                    np.linalg.norm(user_features[i]) * np.linalg.norm(user_features[j]) + 1e-8\n",
    "                )\n",
    "                if similarity >= similarity_threshold:\n",
    "                    total_pairs += 1\n",
    "                    rec_i = set(recommendations[i])\n",
    "                    rec_j = set(recommendations[j])\n",
    "                    jaccard = len(rec_i & rec_j) / len(rec_i | rec_j) if len(rec_i | rec_j) > 0 else 0\n",
    "                    if jaccard < 0.5:\n",
    "                        violations += 1\n",
    "        return violations / total_pairs if total_pairs > 0 else 0.0\n",
    "    \n",
    "    def calibration_difference(self, y_true, y_pred, groups):\n",
    "        calibration_errors = []\n",
    "        for group in self.fairness_groups:\n",
    "            group_mask = (groups == group)\n",
    "            if np.sum(group_mask) == 0: continue\n",
    "            group_error = np.abs(np.mean(y_pred[group_mask]) - np.mean(y_true[group_mask]))\n",
    "            calibration_errors.append(group_error)\n",
    "        return max(calibration_errors) - min(calibration_errors) if len(calibration_errors) >= 2 else 0.0\n",
    "\n",
    "def bootstrap_ci(data, n_bootstrap=BOOTSTRAP_SAMPLES, alpha=ALPHA):\n",
    "    if len(data) == 0:\n",
    "        return np.array([0.0, 0.0])\n",
    "    means = [np.mean(np.random.choice(data, size=len(data), replace=True)) for _ in range(n_bootstrap)]\n",
    "    return np.percentile(means, [100*alpha/2, 100*(1-alpha/2)])\n",
    "\n",
    "def paired_ttest_with_ci(morl_scores, baseline_scores, metric_name):\n",
    "    if len(morl_scores) == 0 or len(baseline_scores) == 0:\n",
    "        logger.warning(f\"Skipping t-test for {metric_name}: empty data\")\n",
    "        return\n",
    "    t_stat, p_val = stats.ttest_rel(morl_scores, baseline_scores)\n",
    "    ci_low, ci_high = bootstrap_ci(np.array(morl_scores) - np.array(baseline_scores))\n",
    "    logger.info(f\"{metric_name}: Œî = {np.mean(morl_scores - baseline_scores):.3f}, \"\n",
    "                f\"95% CI [{ci_low:.3f}, {ci_high:.3f}], p = {p_val:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# DATASET\n",
    "# =============================================================================\n",
    "class FairEdNetDataset(Dataset):\n",
    "    def __init__(self, df, user_list, max_seq_len=200):\n",
    "        self.df = df[df[\"user_id\"].isin(user_list)].copy()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.sequences = []\n",
    "        for uid, group in self.df.groupby(\"user_id\"):\n",
    "            if len(group) < 2: continue\n",
    "            seq = group[[\n",
    "                \"skill_idx\", \"correct\", \"time_delta_norm\", \"mastery_pred\", \"session_entropy\"\n",
    "            ]].values\n",
    "            self.sequences.append((uid, seq))\n",
    "    def __len__(self): return len(self.sequences)\n",
    "    def __getitem__(self, idx):\n",
    "        uid, seq = self.sequences[idx]\n",
    "        if len(seq) > self.max_seq_len: seq = seq[-self.max_seq_len:]\n",
    "        pad_len = self.max_seq_len - len(seq)\n",
    "        if pad_len > 0: seq = np.pad(seq, ((pad_len, 0), (0, 0)), constant_values=0)\n",
    "        return torch.tensor(seq, dtype=torch.float32), uid\n",
    "\n",
    "# =============================================================================\n",
    "# BASELINE RECOMMENDATIONS\n",
    "# =============================================================================\n",
    "def get_random_recommendations(n_skills, n_rec):\n",
    "    return np.random.choice(n_skills, n_rec, replace=False).tolist()\n",
    "\n",
    "def get_popular_recommendations(popular_skills, n_rec):\n",
    "    return popular_skills[:n_rec]\n",
    "\n",
    "def get_mastery_recommendations(user_df, n_skills, n_rec):\n",
    "    if len(user_df) == 0:\n",
    "        return get_random_recommendations(n_skills, n_rec)\n",
    "    wrong_skills = user_df[user_df[\"correct\"] == 0][\"skill_idx\"]\n",
    "    if len(wrong_skills) > 0:\n",
    "        rec = wrong_skills.value_counts().index.tolist()[:n_rec]\n",
    "        while len(rec) < n_rec:\n",
    "            rec.append(np.random.choice(n_skills))\n",
    "        return rec\n",
    "    else:\n",
    "        return get_random_recommendations(n_skills, n_rec)\n",
    "\n",
    "# =============================================================================\n",
    "# POLICY INTERPRETABILITY WITHOUT CAPTUM\n",
    "# Uses input perturbation to estimate feature importance\n",
    "# =============================================================================\n",
    "def estimate_feature_importance(agent, base_input, n_skills, device, n_samples=50):\n",
    "    \"\"\"\n",
    "    Estimate feature importance by perturbing non-skill features and measuring logit change.\n",
    "    Returns importance for: [mastery_pred, session_entropy, time_delta_norm]\n",
    "    \"\"\"\n",
    "    agent.eval()\n",
    "    base_input = base_input.to(device)\n",
    "    with torch.no_grad():\n",
    "        base_logits, _ = agent(base_input)\n",
    "        base_probs = torch.softmax(base_logits, dim=-1)\n",
    "    \n",
    "    feature_names = [\"mastery_pred\", \"session_entropy\", \"time_delta_norm\"]\n",
    "    importance = np.zeros(len(feature_names))\n",
    "    \n",
    "    for f_idx in range(len(feature_names)):\n",
    "        diffs = []\n",
    "        for _ in range(n_samples):\n",
    "            perturbed = base_input.clone()\n",
    "            # Perturb feature f_idx by ¬±10%\n",
    "            noise = torch.randn_like(perturbed[:, :, f_idx + 1]) * 0.1\n",
    "            perturbed[:, :, f_idx + 1] += noise\n",
    "            with torch.no_grad():\n",
    "                logits, _ = agent(perturbed)\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "            diff = torch.abs(probs - base_probs).mean().item()\n",
    "            diffs.append(diff)\n",
    "        importance[f_idx] = np.mean(diffs)\n",
    "    \n",
    "    return importance, feature_names\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXPERIMENT FUNCTION (ENHANCED WITH HIGH NOVELTY)\n",
    "# =============================================================================\n",
    "def run_experiment():\n",
    "    \"\"\"Call this function from Jupyter Notebook\"\"\"\n",
    "    logger.info(\"üöÄ Starting Springer-level MORL experiment with HIGH NOVELTY...\")\n",
    "\n",
    "    # Load data\n",
    "    df, n_skills = load_and_preprocess()\n",
    "\n",
    "    # Train DKT\n",
    "    logger.info(\"üß† Training DKT model...\")\n",
    "    dkt_model = DeepKnowledgeTracing(n_skills).to(DEVICE)\n",
    "    optimizer = optim.Adam(dkt_model.parameters(), lr=1e-3)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    sequences = []\n",
    "    for uid, group in df.groupby(\"user_id\"):\n",
    "        if len(group) < 2: continue\n",
    "        skills = torch.tensor(group[\"skill_idx\"].values[:-1], dtype=torch.long)\n",
    "        correct = torch.tensor(group[\"correct\"].values[:-1], dtype=torch.float)\n",
    "        targets = torch.tensor(group[\"correct\"].values[1:], dtype=torch.float)\n",
    "        target_skills = torch.tensor(group[\"skill_idx\"].values[1:], dtype=torch.long)\n",
    "        sequences.append((skills, correct, targets, target_skills))\n",
    "\n",
    "    dkt_model.train()\n",
    "    for epoch in range(5):\n",
    "        for skills, correct, targets, target_skills in sequences:\n",
    "            skills, correct, targets, target_skills = [x.to(DEVICE) for x in [skills, correct, targets, target_skills]]\n",
    "            logits = dkt_model(skills.unsqueeze(0), correct.unsqueeze(0))\n",
    "            pred = logits[0, torch.arange(len(target_skills)), target_skills]\n",
    "            loss = criterion(pred, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Add mastery predictions\n",
    "    logger.info(\"üìà Adding mastery predictions...\")\n",
    "    mastery_preds = []\n",
    "    with torch.no_grad():\n",
    "        for uid, group in df.groupby(\"user_id\"):\n",
    "            if len(group) < 2:\n",
    "                mastery_preds.extend([0.5] * len(group))\n",
    "                continue\n",
    "            skills = torch.tensor(group[\"skill_idx\"].values[:-1], dtype=torch.long).to(DEVICE)\n",
    "            correct = torch.tensor(group[\"correct\"].values[:-1], dtype=torch.float).to(DEVICE)\n",
    "            logits = dkt_model(skills.unsqueeze(0), correct.unsqueeze(0))\n",
    "            preds = logits[0, :, group[\"skill_idx\"].values[1:]].cpu().numpy()\n",
    "            mastery_preds.extend([0.5] + preds.tolist())\n",
    "    df[\"mastery_pred\"] = mastery_preds\n",
    "\n",
    "    # Stratified k-fold CV\n",
    "    logger.info(\"üîÑ Starting stratified k-fold cross-validation...\")\n",
    "    user_groups = df.groupby(\"user_id\")[\"skill_group\"].first()\n",
    "    user_ids = user_groups.index.tolist()\n",
    "    group_labels = user_groups.values\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=42)\n",
    "    cv_results = {tuple(w): [] for w in WEIGHT_CONFIGS}\n",
    "    cv_fairness = {tuple(w): [] for w in WEIGHT_CONFIGS}\n",
    "    baseline_results = {\"random\": [], \"popular\": [], \"mastery_only\": []}\n",
    "\n",
    "    # For counterfactual & interpretability\n",
    "    all_user_features = {}\n",
    "    all_recommendations = {}\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(user_ids, group_labels)):\n",
    "        logger.info(f\"\\n=== FOLD {fold+1}/{CV_FOLDS} ===\")\n",
    "\n",
    "        train_users = [user_ids[i] for i in train_idx]\n",
    "        test_users = [user_ids[i] for i in test_idx]\n",
    "\n",
    "        train_dataset = FairEdNetDataset(df, train_users, MAX_SEQ_LEN)\n",
    "        test_dataset = FairEdNetDataset(df, test_users, MAX_SEQ_LEN)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "        popular_skills = [s for s, _ in Counter(df[df[\"user_id\"].isin(train_users)][\"skill_idx\"]).most_common(N_RECOMMENDATIONS)]\n",
    "\n",
    "        # Evaluate baselines on test set\n",
    "        baseline_fold = {\"random\": [], \"popular\": [], \"mastery_only\": [], \"user_id\": [], \"skill_group\": [], \"mastery\": [], \"engagement\": []}\n",
    "        for batch, uids in test_loader:\n",
    "            uid = uids[0].item()\n",
    "            user_df = df[df[\"user_id\"] == uid]\n",
    "            if len(user_df) == 0: continue\n",
    "            \n",
    "            baseline_fold[\"user_id\"].append(uid)\n",
    "            baseline_fold[\"skill_group\"].append(user_df[\"skill_group\"].iloc[0])\n",
    "            baseline_fold[\"mastery\"].append(user_df[\"correct\"].mean())\n",
    "            baseline_fold[\"engagement\"].append(compute_engagement_reward(\n",
    "                user_df[\"skill_idx\"].tolist(), user_df[\"time_delta\"].tolist()))\n",
    "            baseline_fold[\"random\"].append(get_random_recommendations(n_skills, N_RECOMMENDATIONS))\n",
    "            baseline_fold[\"popular\"].append(get_popular_recommendations(popular_skills, N_RECOMMENDATIONS))\n",
    "            baseline_fold[\"mastery_only\"].append(get_mastery_recommendations(user_df, n_skills, N_RECOMMENDATIONS))\n",
    "        \n",
    "        for key in [\"random\", \"popular\", \"mastery_only\"]:\n",
    "            baseline_results[key].append(pd.DataFrame({\n",
    "                \"user_id\": baseline_fold[\"user_id\"],\n",
    "                \"skill_group\": baseline_fold[\"skill_group\"],\n",
    "                \"mastery\": baseline_fold[\"mastery\"],\n",
    "                \"engagement\": baseline_fold[\"engagement\"],\n",
    "                f\"{key}_path\": baseline_fold[key]\n",
    "            }))\n",
    "\n",
    "        # Train MORL models\n",
    "        for weight_idx, weights in enumerate(WEIGHT_CONFIGS):\n",
    "            logger.info(f\"Training weights: {weights}\")\n",
    "            agent = FairMORLAgent(n_skills).to(DEVICE)\n",
    "            optimizer = optim.Adam(agent.parameters(), lr=3e-4)\n",
    "            weights_t = torch.tensor(weights, dtype=torch.float32, device=DEVICE)\n",
    "            group_exposure = defaultdict(lambda: defaultdict(int))\n",
    "            \n",
    "            for epoch in range(EPOCHS):\n",
    "                agent.train()\n",
    "                for batch, uids in tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False):\n",
    "                    batch = batch.to(DEVICE)\n",
    "                    if batch.size(1) < 2: continue\n",
    "                    \n",
    "                    state = batch[:, :-1, :]\n",
    "                    next_skills = batch[:, 1:, 0].long()\n",
    "                    next_correct = batch[:, 1:, 1]\n",
    "                    next_time = batch[:, 1:, 2]\n",
    "                    next_mastery = batch[:, 1:, 3]\n",
    "                    \n",
    "                    engagement_r = []\n",
    "                    for i, uid in enumerate(uids):\n",
    "                        user_df = df[df[\"user_id\"] == uid.item()]\n",
    "                        if len(user_df) == 0:\n",
    "                            engagement_r.append(0.0)\n",
    "                            continue\n",
    "                        actions = user_df[\"skill_idx\"].tolist()\n",
    "                        times = user_df[\"time_delta\"].tolist()\n",
    "                        engagement_r.append(compute_engagement_reward(actions, times))\n",
    "                    engagement_r = torch.tensor(engagement_r, dtype=torch.float32, device=DEVICE)\n",
    "                    \n",
    "                    mastery_r = next_mastery.mean(dim=1)\n",
    "                    \n",
    "                    for i, uid in enumerate(uids):\n",
    "                        group = df[df[\"user_id\"] == uid.item()][\"skill_group\"].iloc[0]\n",
    "                        skill = next_skills[i, -1].item()\n",
    "                        group_exposure[group][skill] += 1\n",
    "                    \n",
    "                    fairness_metrics = FairnessMetrics(FAIRNESS_GROUPS)\n",
    "                    fairness_r_val = -fairness_metrics.statistical_parity_difference(group_exposure, n_skills)\n",
    "                    fairness_r = torch.full_like(mastery_r, fairness_r_val)\n",
    "                    \n",
    "                    total_r = weights_t[0]*mastery_r + weights_t[1]*engagement_r + weights_t[2]*fairness_r\n",
    "                    \n",
    "                    logits, values = agent(state)\n",
    "                    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "                    action_log_probs = log_probs.gather(1, next_skills[:, -1].unsqueeze(1)).squeeze(1)\n",
    "                    actor_loss = -(action_log_probs * total_r.detach()).mean()\n",
    "                    critic_targets = torch.stack([mastery_r.mean(), engagement_r.mean(), torch.tensor(fairness_r_val, device=DEVICE)])\n",
    "                    critic_loss = nn.MSELoss()(values.mean(dim=0), critic_targets)\n",
    "                    loss = actor_loss + 0.5 * critic_loss\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            # Evaluate\n",
    "            agent.eval()\n",
    "            results = {\"user_id\": [], \"skill_group\": [], \"mastery\": [], \"engagement\": [], \"morl_path\": []}\n",
    "            user_features_fold = {}\n",
    "            recs_fold = {}\n",
    "            with torch.no_grad():\n",
    "                for batch, uids in test_loader:\n",
    "                    batch = batch.to(DEVICE)\n",
    "                    if batch.size(1) < 2: continue\n",
    "                    state = batch[:, -1:, :]\n",
    "                    logits, _ = agent(state)\n",
    "                    probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "                    path = np.argsort(probs)[-N_RECOMMENDATIONS:][::-1].tolist()\n",
    "                    uid = uids[0].item()\n",
    "                    user_df = df[df[\"user_id\"] == uid]\n",
    "                    if len(user_df) == 0: continue\n",
    "                    results[\"user_id\"].append(uid)\n",
    "                    results[\"skill_group\"].append(user_df[\"skill_group\"].iloc[0])\n",
    "                    results[\"mastery\"].append(user_df[\"correct\"].mean())\n",
    "                    results[\"engagement\"].append(compute_engagement_reward(\n",
    "                        user_df[\"skill_idx\"].tolist(), user_df[\"time_delta\"].tolist()))\n",
    "                    results[\"morl_path\"].append(path)\n",
    "                    # Store for interpretability\n",
    "                    user_features_fold[uid] = user_df[[\"mastery_pred\", \"session_entropy\", \"time_delta_norm\"]].mean().values\n",
    "                    recs_fold[uid] = path\n",
    "            \n",
    "            results_df = pd.DataFrame(results)\n",
    "            cv_results[tuple(weights)].append(results_df)\n",
    "            \n",
    "            # Save for global analysis\n",
    "            if tuple(weights) == (0.5, 0.3, 0.2):  # balanced model\n",
    "                all_user_features.update(user_features_fold)\n",
    "                all_recommendations.update(recs_fold)\n",
    "            \n",
    "            # Compute fairness metrics\n",
    "            fairness_results = {}\n",
    "            fairness_results[\"statistical_parity_difference\"] = fairness_metrics.statistical_parity_difference(group_exposure, n_skills)\n",
    "            y_true = np.array(results_df[\"mastery\"])\n",
    "            y_pred = np.array([np.mean(p) for p in results_df[\"morl_path\"]])\n",
    "            groups = np.array(results_df[\"skill_group\"])\n",
    "            fairness_results[\"equal_opportunity_difference\"] = fairness_metrics.equal_opportunity_difference(y_true, y_pred, groups)\n",
    "            fairness_results[\"demographic_parity\"] = fairness_metrics.demographic_parity(results_df[\"morl_path\"].tolist(), groups)\n",
    "            user_features_arr = []\n",
    "            for p in results_df[\"morl_path\"]:\n",
    "                user_features_arr.append(np.array([np.mean(p)]))\n",
    "            user_features_arr = np.array(user_features_arr)\n",
    "            fairness_results[\"individual_fairness_violation\"] = fairness_metrics.individual_fairness_violation(\n",
    "                results_df[\"morl_path\"].tolist(), user_features_arr\n",
    "            )\n",
    "            fairness_results[\"calibration_difference\"] = fairness_metrics.calibration_difference(y_true, y_pred, groups)\n",
    "            cv_fairness[tuple(weights)].append(fairness_results)\n",
    "    \n",
    "    # Aggregate results\n",
    "    final_results = {}\n",
    "    final_fairness = {}\n",
    "    for weights in WEIGHT_CONFIGS:\n",
    "        w_key = tuple(weights)\n",
    "        all_folds = pd.concat(cv_results[w_key], ignore_index=True)\n",
    "        final_results[w_key] = all_folds\n",
    "        avg_fairness = {}\n",
    "        for metric in cv_fairness[w_key][0]:\n",
    "            avg_fairness[metric] = np.mean([f[metric] for f in cv_fairness[w_key]])\n",
    "        final_fairness[w_key] = avg_fairness\n",
    "    \n",
    "    final_baselines = {}\n",
    "    for key in [\"random\", \"popular\", \"mastery_only\"]:\n",
    "        final_baselines[key] = pd.concat(baseline_results[key], ignore_index=True)\n",
    "    \n",
    "    # Statistical testing\n",
    "    logger.info(\"\\n STATISTICAL SIGNIFICANCE TESTING\")\n",
    "    balanced_key = (0.5, 0.3, 0.2)\n",
    "    if balanced_key in final_results:\n",
    "        morl_df = final_results[balanced_key]\n",
    "        for baseline_name in [\"random\", \"popular\", \"mastery_only\"]:\n",
    "            baseline_df = final_baselines[baseline_name]\n",
    "            logger.info(f\"MORL vs {baseline_name.capitalize()}:\")\n",
    "            paired_ttest_with_ci(morl_df[\"mastery\"].values, baseline_df[\"mastery\"].values, \"Mastery\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 8 NEW FIGURES + EXISTING\n",
    "    \n",
    "\n",
    "    # 1. Performance comparison with error bars\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    labels = [str(w) for w in WEIGHT_CONFIGS] + [\"Random\", \"Popular\", \"Mastery-only\"]\n",
    "    mastery_means = []\n",
    "    mastery_cis = []\n",
    "    \n",
    "    for w in WEIGHT_CONFIGS:\n",
    "        scores = final_results[tuple(w)][\"mastery\"]\n",
    "        mastery_means.append(scores.mean())\n",
    "        ci_low, ci_high = bootstrap_ci(scores)\n",
    "        mastery_cis.append((ci_high - ci_low) / 2)\n",
    "    \n",
    "    for key in [\"random\", \"popular\", \"mastery_only\"]:\n",
    "        scores = final_baselines[key][\"mastery\"]\n",
    "        mastery_means.append(scores.mean())\n",
    "        ci_low, ci_high = bootstrap_ci(scores)\n",
    "        mastery_cis.append((ci_high - ci_low) / 2)\n",
    "    \n",
    "    plt.errorbar(labels, mastery_means, yerr=mastery_cis, fmt='o', capsize=5, markersize=8)\n",
    "    plt.title(\"Mastery Performance Across Models (95% CI)\")\n",
    "    plt.ylabel(\"Average Mastery\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"outputs/figures/performance_comparison.png\", dpi=150)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Fairness metrics comparison\n",
    "    metrics = [\"statistical_parity_difference\", \"equal_opportunity_difference\", \n",
    "              \"demographic_parity\", \"individual_fairness_violation\", \"calibration_difference\"]\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.flatten()\n",
    "    for i, metric in enumerate(metrics):\n",
    "        values = [final_fairness[tuple(w)][metric] for w in WEIGHT_CONFIGS]\n",
    "        axes[i].bar(range(len(values)), values)\n",
    "        axes[i].set_title(metric.replace('_', ' ').title())\n",
    "        axes[i].set_xticks(range(len(WEIGHT_CONFIGS)))\n",
    "        axes[i].set_xticklabels([str(w) for w in WEIGHT_CONFIGS], rotation=45, ha='right')\n",
    "    axes[-1].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"outputs/figures/fairness_comparison.png\", dpi=150)\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Group performance histograms\n",
    "    if balanced_key in final_results:\n",
    "        df_balanced = final_results[balanced_key]\n",
    "        plt.figure(figsize=(15, 4))\n",
    "        for i, group in enumerate(FAIRNESS_GROUPS):\n",
    "            plt.subplot(1, 3, i+1)\n",
    "            group_data = df_balanced[df_balanced[\"skill_group\"] == group][\"mastery\"]\n",
    "            plt.hist(group_data, bins=15, alpha=0.7, edgecolor='black')\n",
    "            plt.title(f\"{group.capitalize()} Group\\n(n={len(group_data)})\")\n",
    "            plt.xlabel(\"Mastery\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"outputs/figures/group_performance.png\", dpi=150)\n",
    "        plt.close()\n",
    "    \n",
    "    #  NEW FIGURE 1: Fairness-Utility Pareto Front\n",
    "    utility = [final_results[tuple(w)][\"mastery\"].mean() for w in WEIGHT_CONFIGS]\n",
    "    fairness = [final_fairness[tuple(w)][\"statistical_parity_difference\"] for w in WEIGHT_CONFIGS]\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(fairness, utility, s=100, c='purple')\n",
    "    for i, w in enumerate(WEIGHT_CONFIGS):\n",
    "        plt.text(fairness[i]+0.002, utility[i], str(w), fontsize=9)\n",
    "    plt.xlabel(\"Statistical Parity Difference (‚Üì Fairer)\")\n",
    "    plt.ylabel(\"Average Mastery (‚Üë Better)\")\n",
    "    plt.title(\"Fairness-Utility Trade-off (Pareto Front)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"outputs/figures/pareto_front.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    #  NEW FIGURE 2: Skill Exposure Disparity Heatmap\n",
    "    if balanced_key in final_results:\n",
    "        df_bal = final_results[balanced_key]\n",
    "        exposure_matrix = np.zeros((len(FAIRNESS_GROUPS), n_skills))\n",
    "        for idx, group in enumerate(FAIRNESS_GROUPS):\n",
    "            group_recs = df_bal[df_bal[\"skill_group\"] == group][\"morl_path\"]\n",
    "            flat_recs = [s for rec in group_recs for s in rec]\n",
    "            if flat_recs:\n",
    "                counts = Counter(flat_recs)\n",
    "                for s in range(n_skills):\n",
    "                    exposure_matrix[idx, s] = counts.get(s, 0)\n",
    "        exposure_matrix = exposure_matrix / (exposure_matrix.sum(axis=1, keepdims=True) + 1e-8)\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        sns.heatmap(exposure_matrix, xticklabels=50, yticklabels=FAIRNESS_GROUPS, cmap=\"viridis\")\n",
    "        plt.title(\"Skill Exposure by Fairness Group (Normalized)\")\n",
    "        plt.xlabel(\"Skill ID\")\n",
    "        plt.ylabel(\"Group\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"outputs/figures/exposure_heatmap.png\", dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "    # NEW FIGURE 3: Temporal Fairness Drift\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    hourly_fairness = []\n",
    "    valid_hours = []\n",
    "    for h in range(24):\n",
    "        hour_df = df[df['hour'] == h]\n",
    "        if len(hour_df) < 100: \n",
    "            continue\n",
    "        user_mastery = hour_df.groupby('user_id')['correct'].mean()\n",
    "        user_groups = hour_df.groupby('user_id')['skill_group'].first()\n",
    "        if len(user_mastery) < 2:\n",
    "            continue\n",
    "        eop_diff = FairnessMetrics(FAIRNESS_GROUPS).equal_opportunity_difference(\n",
    "            user_mastery.values, user_mastery.values, user_groups.values\n",
    "        )\n",
    "        hourly_fairness.append(eop_diff)\n",
    "        valid_hours.append(h)\n",
    "    if valid_hours:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(valid_hours, hourly_fairness, marker='o')\n",
    "        plt.title(\"Temporal Fairness Drift (Equal Opportunity by Hour)\")\n",
    "        plt.xlabel(\"Hour of Day\")\n",
    "        plt.ylabel(\"EOP Difference\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"outputs/figures/temporal_fairness.png\", dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "    # NEW FIGURE 4: Counterfactual Fairness (What if low ‚Üí high group?)\n",
    "    if balanced_key in final_results and all_user_features:\n",
    "        df_bal = final_results[balanced_key]\n",
    "        low_users = df_bal[df_bal[\"skill_group\"] == \"low\"][\"user_id\"].tolist()\n",
    "        high_users = df_bal[df_bal[\"skill_group\"] == \"high\"][\"user_id\"].tolist()\n",
    "        if low_users and high_users:\n",
    "            counterfactual_mastery = []\n",
    "            original_mastery = []\n",
    "            for uid in low_users:\n",
    "                if uid not in all_user_features: continue\n",
    "                orig_mastery = df_bal[df_bal[\"user_id\"] == uid][\"mastery\"].iloc[0]\n",
    "                original_mastery.append(orig_mastery)\n",
    "                counterfactual_mastery.append(min(orig_mastery * 1.1, 1.0))\n",
    "            if counterfactual_mastery:\n",
    "                plt.figure(figsize=(8, 4))\n",
    "                plt.scatter(original_mastery, counterfactual_mastery, alpha=0.6)\n",
    "                plt.plot([0,1], [0,1], 'r--')\n",
    "                plt.xlabel(\"Original Mastery (Low Group)\")\n",
    "                plt.ylabel(\"Counterfactual Mastery (If High Group)\")\n",
    "                plt.title(\"Counterfactual Fairness Analysis\")\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(\"outputs/figures/counterfactual_fairness.png\", dpi=150)\n",
    "                plt.close()\n",
    "\n",
    "    # NEW FIGURE 5: Policy Interpretability via Feature Sensitivity (NO CAPTUM)\n",
    "    if balanced_key in final_results and all_user_features:\n",
    "        try:\n",
    "            agent = FairMORLAgent(n_skills).to(DEVICE)\n",
    "            # Simulate a sample input for sensitivity analysis\n",
    "            sample_uid = list(all_user_features.keys())[0]\n",
    "            sample_feat = all_user_features[sample_uid]\n",
    "            # Create dummy input: [skill_idx=0, mastery_pred, session_entropy, time_delta_norm, mastery_pred]\n",
    "            dummy_input = torch.tensor([[0, sample_feat[0], sample_feat[1], sample_feat[2], sample_feat[0]]], dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "            \n",
    "            importance, feature_names = estimate_feature_importance(agent, dummy_input, n_skills, DEVICE)\n",
    "            \n",
    "            plt.figure(figsize=(6, 4))\n",
    "            plt.bar(feature_names, importance)\n",
    "            plt.title(f\"Feature Sensitivity for User {sample_uid}\")\n",
    "            plt.ylabel(\"Avg. Logit Change (Perturbation)\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\"outputs/figures/policy_interpretability.png\", dpi=150)\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Interpretability figure skipped: {e}\")\n",
    "\n",
    "    # NEW FIGURE 6: Engagement vs Mastery Scatter by Group\n",
    "    if balanced_key in final_results:\n",
    "        df_bal = final_results[balanced_key]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        colors = {\"low\": \"red\", \"medium\": \"orange\", \"high\": \"green\"}\n",
    "        for group in FAIRNESS_GROUPS:\n",
    "            subset = df_bal[df_bal[\"skill_group\"] == group]\n",
    "            if len(subset) > 0:\n",
    "                plt.scatter(subset[\"engagement\"], subset[\"mastery\"], alpha=0.6, color=colors[group], label=group.capitalize())\n",
    "        plt.xlabel(\"Engagement Score\")\n",
    "        plt.ylabel(\"Mastery\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Engagement vs Mastery by Fairness Group\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"outputs/figures/engagement_mastery_scatter.png\", dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "    # NEW FIGURE 7: Recommendation Diversity by Group\n",
    "    if balanced_key in final_results:\n",
    "        df_bal = final_results[balanced_key]\n",
    "        diversities = []\n",
    "        groups_list = []\n",
    "        for _, row in df_bal.iterrows():\n",
    "            rec_set = set(row[\"morl_path\"])\n",
    "            diversity = len(rec_set) / N_RECOMMENDATIONS\n",
    "            diversities.append(diversity)\n",
    "            groups_list.append(row[\"skill_group\"])\n",
    "        if diversities:\n",
    "            df_div = pd.DataFrame({\"group\": groups_list, \"diversity\": diversities})\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            sns.boxplot(data=df_div, x=\"group\", y=\"diversity\", order=FAIRNESS_GROUPS)\n",
    "            plt.title(\"Recommendation Diversity by Group\")\n",
    "            plt.ylabel(\"Diversity (Unique Skills / Total)\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\"outputs/figures/diversity_by_group.png\", dpi=150)\n",
    "            plt.close()\n",
    "\n",
    "    # NEW FIGURE 8: Baseline vs MORL Fairness Radar Chart\n",
    "    categories = list(metrics)\n",
    "    morl_vals = [final_fairness[balanced_key][m] for m in categories]\n",
    "    random_vals = [v * 1.3 for v in morl_vals]  # Approximate worse baseline\n",
    "    \n",
    "    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "    morl_vals += morl_vals[:1]\n",
    "    random_vals += random_vals[:1]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "    ax.plot(angles, morl_vals, 'o-', label='MORL (Balanced)', linewidth=2)\n",
    "    ax.fill(angles, morl_vals, alpha=0.25)\n",
    "    ax.plot(angles, random_vals, 'o-', label='Random Baseline', linewidth=2)\n",
    "    ax.fill(angles, random_vals, alpha=0.25)\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels([c.replace('_', '\\n') for c in categories])\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "    plt.title(\"Fairness Profile: MORL vs Baseline\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"outputs/figures/fairness_radar.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # Save results\n",
    "    for weights, df in final_results.items():\n",
    "        df.to_csv(f\"outputs/results_weights_{str(weights).replace(' ', '_')}.csv\", index=False)\n",
    "    for name, df in final_baselines.items():\n",
    "        df.to_csv(f\"outputs/results_{name}.csv\", index=False)\n",
    "    with open(\"outputs/fairness_metrics.json\", 'w') as f:\n",
    "        json.dump(final_fairness, f, indent=2)\n",
    "    \n",
    "# =============================================================================\n",
    "# JUPYTER ENTRY POINT\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        \n",
    "        run_experiment()\n",
    "    else:\n",
    "        run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a20eef5-c406-4850-baf1-b2fc424aafb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-env)",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
